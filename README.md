# TP Intergiciel - Kafka

Auteurs : ***Maxence ZOLNIERUCK*** & ***Josue VIDREQUIN***

## Sommaire

- [TP Intergiciel - Kafka](#tp-intergiciel---kafka)
  - [Sommaire](#sommaire)
  - [Introduction](#introduction)
  - [Commandes](#commandes)
  - [D√©marrer les services](#d√©marrer-les-services)
    - [Topics Kafka](#topics-kafka)
    - [Base de donn√©es PostgreSQL](#base-de-donn√©es-postgresql)
    - [Consumers & Producers Kafa Java](#consumers--producers-kafa-java)
      - [Probl√®me possible](#probl√®me-possible)
  - [Architecture applicative](#architecture-applicative)
    - [Consumers et Producers](#consumers-et-producers)
      - [Producer n¬∞1 : R√©cup√©ration des donn√©es](#producer-n1--r√©cup√©ration-des-donn√©es)
      - [Consumer n¬∞1 : Stockage en base de donn√©es](#consumer-n1--stockage-en-base-de-donn√©es)
      - [Producer n¬∞2 & Consumer n¬∞3 : Interface client et saisie de commandes](#producer-n2--consumer-n3--interface-client-et-saisie-de-commandes)
      - [Consumer n¬∞2 & Producer n¬∞3 : Interrogation de la base de donn√©es et calculs](#consumer-n2--producer-n3--interrogation-de-la-base-de-donn√©es-et-calculs)
    - [Base de donn√©es](#base-de-donn√©es)
    - [Format des donn√©es](#format-des-donn√©es)

## Introduction

Ce d√©p√¥t GitHub fait office de rendu de projet dans le cadre du module Intergiciel √† l'INSA Hauts-de-France. Le but de ce projet est d'utiliser un bus de messages Kafka afin de faire communiquer plusieurs projets Java. L'application finale est utilisable via ligne de commandes et restitue plusieurs informations et statistiques calcul√©s gr√¢ce aux donn√©es de l'API publique <https://api.covid19api.com>.

> Vous pouvez visualiser les donn√©es de cette API dans ce [fichier](covid_api.json).

Parmis les valeurs que nous restituons, nous retrouvons : le nombre total de cas confirm√©s, le nombre total de personnes d√©c√©d√©es, le nombre de nouveaux cas, le nombre de nouveaux d√©c√®s et enfin, le nombre de personnes r√©tablies. Ces donn√©es sont regroup√©es soit par pays, soit √† l'international avec des valeurs cumul√©es.

Toutes les donn√©es (sauf pour l'export) sont retourn√©es √† l'utilisateur au format Json.

## Commandes

Voici les commandes support√©es par l'application :

- **help** : affiche les commandes disponibles.
- **get_global_values** : affiche les valeurs cumul√©es de tous les pays.
- **get_country_values V_PAYS** : affiche les donn√©es sp√©cifiques √† un pays. Exemple : V_PAYS=FR
- **get_confirmed_avg** : affiche une moyenne des cas confirm√©s.
- **get_deaths_avg** : affiche une moyenne des d√©c√®s.
- **get_countries_deaths_percent** : affiche le pourcentage de d√©c√®s par rapport aux cas confirm√©s par pays.
- **export** : affiche une extraction de la base de donn√©es au format XML.

Ci dessous, le r√©sultat de l'ex√©cution de la commande **help** :

![Commande Help](./Images/commande_help.png)

Un exemple de retour de commande avec **get_country_values FR** :

![Exemple get_countr_values](./Images/get_country_values-exemple.png)

## D√©marrer les services

Le projet est compos√© grossi√®rement de trois blocs :

1. Les topics Kafka
2. La base de donn√©es PostgreSQL
3. Les projets Java (Consumers & Producers Kafka)

Nous allons voir comment lancer le projet, pas √† pas. Il y a √† votre disposition des [Scripts](./Scripts) qui vous aideront dans ce processus.

Il est important de suivre les √©tapes dans l'ordre car il y a une certaine logique dans le branchement des diff√©rentes parties.

### Topics Kafka

Tout d'abord, il vous faut avoir install√© Kafka sur votre PC. Pour notre part, nous l'avons install√© sur Windows 11 sur `C:\Kafka`.

Si vous rencontrez des probl√®mes pour l'installation de Kafka, nous vous recommandons ce tutoriel que nous avons suivi pour ce TP : <https://www.geeksforgeeks.org/how-to-install-and-run-apache-kafka-on-windows/>

Une fois Kafka install√©, vous pouvez utiliser le script [demarrage_kafka_zookeeper.bat](./Scripts/demarrage_kafka_zookeeper.bat) afin de d√©marrer les services `Kafka` et `Zookeeper`.

Maintenant, nous allons passer √† la cr√©ation des topics qui seront utilis√©s par les applications Java. Pour cela, le script [creation_topics_kafka.bat](./Scripts/creation_topics_kafka.bat) a √©t√© cr√©√©.

Finalement, apr√®s avoir d√©marr√© Kafka et cr√©√© les topics, nous pouvons tester le bon fonctionnement des scripts avec la commande :

> C:\Kafka\bin\windows\kafka-topics.bat --bootstrap-server localhost:9092 --describe --topic TOPIC_NAME

O√π TOPIC_NAME peut prendre les valeurs : topic1, topic2 et topic3.

Bien, nous avons pass√© la premi√®re √©tape, int√©ressons nous maintenant √† la base de donn√©es PostgreSQL.

### Base de donn√©es PostgreSQL

Pour ce projet nous utilisons une base de donn√©es PostgreSQL car elle permet de stocker des valeurs au format Json directement. Chose tr√®s utile pour nous qui utilisons une API publique nous retournant les valeurs au format Json. De ce fait, apr√®s quelques traitements, nous pouvons directement stocker le resultat de l'appel dans la base de donn√©es.

Nous avons utilis√© une image docker pour heberger notre base de donn√©es. Vous pouvez retrouver le [docker-compose.yml](docker-compose.yml) a la racine de ce rep√©rtoire pour lancer vous m√™me l'image docker.

La base de donn√©es sera accessible via l'adresse `http://localhost:5432/HOPSIIA`.

Ensuite, vous pouvez utiliser l'outil [pgAdmin](https://www.pgadmin.org/) mis √† disposition par PostgreSQL afin d'administrer vos bases de donn√©es.

![pgAdmin DB](./Images/pgadmin_db.png)

Maintenant que la base de donn√©es a √©t√© cr√©√©e, il faut instancier la table qui accueillera les donn√©es de l'API Covid-19. Pour cela, vous avez √† votre disposition le script [creation_table_postgres.sql](./Scripts/creation_table_postgres.sql) dans ce d√©pot pour la cr√©ation de la table **covid**.

Ensuite, il faut initialiser la table avec un enregistrement vide. Pour cela, ex√©cutez la requ√™te qui se trouve dans [initialiser_table_postgres.sql](./Scripts/initialiser_table_postgres.sql).

Le second point de ce projet est pr√™t. Cette base de donn√©es sera aliment√©e gr√¢ce √† deux projets Java : Cs1 (Consumer 1) et Pr1 (Producer 1) que nous allons aborder d√®s maintenant.

### Consumers & Producers Kafa Java

Les modules Java sont accessibles dans le dossier [Sources](./Sources/).

Il faut imp√©rativement d√©marrer les modules dans cet ordre :

1. Producer n¬∞1 ([Pr1](./Sources/pr1/))
2. Consumer n¬∞1 ([Cs1](./Sources/cs1/))
3. Consumer n¬∞2 & Producer n¬∞3 ([Cs2Pr3](./Sources/cs2pr3/))
4. Producer n¬∞2 & Consumer n¬∞3 ([Pr2Cs3](./Sources/pr2cs3/))

Vous pouvez les importer dans le workspace de votre IDE favori (pour nous c'est [Eclipse](https://www.eclipse.org/ide/) üíú) et les lancer dans l'ordre d√©crit ci-dessus.

Tous les projets ont une classe portant comme nom `XXX`MainClass o√π `XXX` est le nom du projet. Par exemple : [Pr1MainClass.java](./Sources/pr1/src/main/java/pr1/Pr1MainClass.java). Ce sera ces classes qu'il faut d√©marrer pour lancer le module.

#### Probl√®me possible

Il est possible que l'export des donn√©es n'affiche rien dans la console Eclipse. Si cela est le cas, c'est car Eclipse limite la taille des messages √† afficher.

Pour modifier ce param√®tre, aller dans `Window > Preferences > Run/Debug > Console` puis red√©finissez les param√®tres tels que :

![Param√®tres Eclipse](./Images/parametre_console_eclipse.png)

## Architecture applicative

### Consumers et Producers

Les consumers (Cs) et Producers (Pr) sont des applications Kafka permetant d'√©crire dans un topic pour un Pr et de lire les messages pour un Cs. Dans notre projet, il y a 4 "briques" :

- Pr1 : Lit les messages de l'API Covid
- Cs1 : Mets √† jour la base de donn√©es
- Pr2Cs3 : L'invit√© de commande utilis√© pour saisir les commandes
- Cs2Pr3 : Communique avec la base de donn√©es et effectue des traitements sp√©cifiques √† la demande utilisateur re√ßue

Ces briques Kafka communiquent entre elles via les topics cr√©√©s pr√©c√©demment √† la section [Topics Kafa](#topics-kafka).

Pour un peu plus de claret√©, voici un sch√©ma de l'architecture applicative de notre projet :

![Architecture Applicative](./Images/architecture_applicative.png)

#### Producer n¬∞1 : R√©cup√©ration des donn√©es

Ce projet fait un appel automatis√© toutes les 30 minutes via la librairie [Quartz Scheduler](http://www.quartz-scheduler.org/) √† l'[API Covid-19](https://api.covid19api.com). A chaque fois que le projet r√©cup√®re les donn√©es de l'API, nous effectuons un l√©ger traitement pour √©purer le JSON en supprimant certains champs qui ne nous int√©ressent pas ici. Ensuite, le Json nouvellement cr√©√© est envoy√© dans le topic n¬∞1.

![Partie Producer n¬∞1](./Images/partie_pr1.png)

#### Consumer n¬∞1 : Stockage en base de donn√©es

Du c√¥t√© Consumer n¬∞1, le module est en attente constante de nouveaux messages √† lire dans le topic n¬∞1. Quand un message est d√©tect√©, le module en r√©cup√®re le contenu et effectue une connexion √† la base de donn√©es pour envoyer la mise √† jour des donn√©es.

![Partie Consumer n¬∞1](./Images/partie_cs1.png)

#### Producer n¬∞2 & Consumer n¬∞3 : Interface client et saisie de commandes

Le module le plus lourd est celui-ci √©tant donn√© qu'il embarque √† la fois : une partie consumer, une partie producer et une partie invit√© de commande pour que l'utilisateur saisisse ses requ√™tes. Cependant, son fonctionnement est assez simple. Apr√®s avoir demand√© √† l'utilisateur la commande qu'il souhaite ex√©cuter, le module v√©rifie que celle-ci est bien valide et qu'elle pourra √™tre ex√©cut√©e c√¥t√© [Cs2Pr3](#cs2pr3) sans aucun soucis. Ensuite, la commande est envoy√©e via le topic n¬∞2 et sera ex√©cut√©e par l'autre module. Lorsque la requ√™te aura √©t√© ex√©cut√©e, notre module r√©cup√®rera le r√©sultat dans le topic n¬∞3 afin de l'afficher √† l'√©cran pour l'utilisateur.

![Partie Producer n¬∞2 & Consumer n¬∞3](./Images/partie_pr2cs3.png)

#### Consumer n¬∞2 & Producer n¬∞3 : Interrogation de la base de donn√©es et calculs

Ce module fait office d'interface avec la base de donn√©e et de brique de calcul. En effet, ce module lit des commandes utilisateurs √† travers le topic n¬∞2 qu'il transcode en requ√™te SQL pour int√©rroger la base de donn√©es. Une fois les donn√©es re√ßues, un enchainement de calculs est ex√©cut√© pour r√©pondre √† la demande utilisateur. Enfin, ce resultat est communiqu√© au module appelant via le topic n¬∞3.

![Partie Consumer n¬∞2 & Producer n¬∞3](./Images/partie_cs2pr3.png)

### Base de donn√©es

La base de donn√©es PostgreSQL comporte une unique table appel√©e `covid`. Dans cette table se trouve trois colonnes :

| covid.id         | covid.data               | covid.date_update                                  |
|------------------|--------------------------|----------------------------------------------------|
| identifiant fixe | donn√©es de l'API en Json | Date et heure √† la quelle la BDD a √©t√© mise √† jour |

Lorsque le [Cs1](#consumer-n1--stockage-en-base-de-donn√©es) souhaite effectuer une mise √† jour de la base de donn√©es, il va dans les faits venir √©craser le seul enregistrement. Pour cela, l'enregistrement poss√®de un identifiant unique : `covid.id = 1`.

De ce fait, la requ√™te SQL ex√©cut√©e par le [Cs1](#consumer-n1--stockage-en-base-de-donn√©es) pour mettre √† jour la base de donn√©es est la suivante :

> UPDATE public.covid SET data=`?1`, date_update=`?2` WHERE **id=1**;

Nous voyons bien que les donn√©es contenues dans l'enregistrement portant l'identifiant n¬∞1 seront √©cras√©es par les nouvelles.

Le champ `data` dans la base de donn√©es est au format [JsonB](https://www.postgresql.org/docs/13/datatype-json.html). Cela nous permet d'ex√©cuter des requ√™tes tout en profitant des avantages du Json. Par exemple, pour la commande `get_country_values V_PAYS`, voici le type de requ√™te ex√©cut√©e :

![Exemple JsonB](./Images/requete_jsonb.png)

### Format des donn√©es

Les donn√©es de l'API ne sont consid√©r√©es comme Json uniquement √† partir du moment o√π elles sont stock√©es en base de donn√©es. Avant cela, dans le Producer n¬∞1, dans le topic n¬∞1 et dans le Consumer n¬∞1, l'appel √† l'API Covid-19 est r√©cup√©r√© comme une chaine de caract√®res √† part enti√®re. Le fait de le convertir en Json une foi en base de donn√©es nous permet d'effectuer des requ√™tes plus performantes et plus pr√©cises. Cela nous √©vite √©galement de la compl√©xit√© au niveau des modules Pr1 et Cs1.
